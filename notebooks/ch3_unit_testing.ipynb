{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae6580a4",
   "metadata": {},
   "source": [
    "# Chapter 3: Unit Testing\n",
    "\n",
    "## What is Unit Testing?\n",
    "\n",
    "Unit testing is a common software testing technique to check individual\n",
    "code components (e.g., functions, methods, classes, or modules) in isolation from\n",
    "the rest of the program.\n",
    "\n",
    "The main idea is to run such isolated components with a variety of inputs and check the\n",
    "outputs against expected results.\n",
    "\n",
    "For unit testing to be achievable and effective, the code design must facilitate\n",
    "easy isolation of components and their dependencies. In line with the design\n",
    "principles we discussed earlier, below are some key practices to follow:\n",
    "\n",
    " - **Small, cohesive functions** \n",
    "\n",
    " - **Explicit interfaces** \n",
    "\n",
    " - **Avoid side effects** \n",
    "\n",
    " - **Use dependency injection** \n",
    "\n",
    " ## Benefits of Unit Testing\n",
    "\n",
    " - **Catches bugs early**\n",
    "\n",
    " - **Facilitates code changes** \n",
    "\n",
    " - **Improves code design** \n",
    " - **Documentation** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ad44dd",
   "metadata": {},
   "source": [
    "\n",
    "## Unit Testing in Action\n",
    "\n",
    "Take the `diffusive_flux` function from the previous chapter as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38c35ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = list[float]\n",
    "\n",
    "def diffusive_flux(f_out: vec, c: vec, kappa: float, dx: float) -> None:\n",
    "    \"\"\"Given a cell field (c), compute the diffusive flux (f_out).\"\"\"\n",
    "    assert len(f_out) == len(c) + 1, \"Size mismatch\"\n",
    "    assert dx > 0 and kappa > 0, \"Non-positive dx or kappa\"\n",
    "    for i in range(1, len(f_out) - 1):\n",
    "        f_out[i] = -kappa * (c[i] - c[i-1]) / dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb3570",
   "metadata": {},
   "source": [
    "Unit testing this function is as simple as calling it with some test inputs and checking the outputs. Here's how you might write a unit test for the `diffusive_flux` function using `pytest`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f400961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import isclose\n",
    "\n",
    "def test_diffusive_flux():\n",
    "    \"\"\"Constant field leads to zero flux\"\"\"\n",
    "    u = [100.0, 100.0, 100.0]\n",
    "    F = [0.0] * (len(u) + 1)\n",
    "\n",
    "    diffusive_flux(F, u, kappa=0.1, dx=1.0)\n",
    "    assert all(isclose(F[:], 0)), f\"Expected all zeros, got {F[1:-1]}\"\n",
    "\n",
    "test_diffusive_flux()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e3926b",
   "metadata": {},
   "source": [
    "That's it! We unit tested the `diffusive_flux` function. And the test passed successfully.\n",
    "How do we know? The assertion did not raise an error.\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary><b>What are some of the limitations of this test?</b> </summary>\n",
    "  \n",
    " We only did so for a specific case, where we confirmed that a constant field \n",
    "and zero boundary fluxes lead to zero flux. We'll have to add more tests to cover different scenarios,\n",
    "but first let's introduce the `pytest` framework, which makes writing and running tests easier.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571498fa",
   "metadata": {},
   "source": [
    "## The `pytest` Library\n",
    "\n",
    "While manual testing is useful, it can be **time-consuming** and **error-prone**. Automated testing with a framework like `pytest` allows us to quickly and easily run our tests, for instance, when added into a continuous integration (CI) pipeline.\n",
    "\n",
    "While there are other commonly used frameworks such as `unittest`, we prefer\n",
    "`pytest` for its simplicity and powerful features. It's also worth noting that `pytest`\n",
    "is not just for unit testing: it's a general purpose testing framework that can be used\n",
    "for a wide range of testing needs in an automated fashion.\n",
    "\n",
    "Note that `pytest` is a command-line tool. As such, we will follow the following\n",
    "workflow to run our tests in a Jupyter notebook environment:\n",
    "\n",
    "1. Using the `%%writefile` magic command, we will save our test codes to Python files.\n",
    "2. We will then run the tests using the command line command `pytest`. Recall, in Jupyter notebooks, we can run such shell commands by prefixing them with `!`.\n",
    "\n",
    "But first, let's load the solver code that was saved in the previous chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3067e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load heat1d.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773035ee",
   "metadata": {},
   "source": [
    "## Key Features of `pytest`\n",
    "\n",
    "Here, we highlight some of the key features of `pytest`:\n",
    "\n",
    " - Assertion\n",
    " - Test discovery\n",
    " - Fixtures\n",
    " - Marking\n",
    " - Parameterization\n",
    "\n",
    "\n",
    "### Assertions\n",
    "\n",
    "pytest uses plain Python assert statements, no special API, to decide whether a test passes.\n",
    "When an assertion fails, pytest reports the failure along with the values of the\n",
    "expressions involved.\n",
    "\n",
    "In cases where you have property specifications (preconditions, postconditions, invariants)\n",
    "specified as part of the actual code, you can automatically leverage them in your tests.\n",
    "Otherwise, you can implement them as asserts that precede or follow the function under test.\n",
    "\n",
    "Be careful with floating-point comparisons: exact equality is brittle. In tests, \n",
    "prefer `pytest.approx` for tolerant comparisons. Using `numpy.isclose` inside your\n",
    "library code is preferable in production code. But for test assertions,\n",
    "`approx` tends to produce clearer failure messages.\n",
    "\n",
    "Let’s write a minimal test for the `div` function. In practice, source code and test are \n",
    "often located in separate files. But for brevity, we’ll keep them together and save to\n",
    "`test_div.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04219b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_div.py\n",
    "\n",
    "def div(x, y):\n",
    "    assert y != 0           # P    (precondition)\n",
    "    res = x / y             # code (implementation)\n",
    "    assert res * y == x     # Q    (postcondition)\n",
    "    return res\n",
    "\n",
    "def test_division():\n",
    "    div(7, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f64a60",
   "metadata": {},
   "source": [
    "To run this test, we can simply run the `pytest test_div.py` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9530cd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest test_div.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d069e85a",
   "metadata": {},
   "source": [
    "Notice the test fails because the assertion is violated due to floating point precision of division operation. We may address this test failure in several ways. First, we can use the `raises` context manager:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee3bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_div.py\n",
    "\n",
    "from pytest import raises\n",
    "\n",
    "def div(x, y):\n",
    "    assert y != 0           # P    (precondition)\n",
    "    res = x / y             # code (implementation)\n",
    "    assert res * y == x     # Q    (postcondition)\n",
    "    return res\n",
    "\n",
    "def test_division():\n",
    "    with raises(AssertionError):\n",
    "        res = div(7, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952cbaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest test_div.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7452eaf",
   "metadata": {},
   "source": [
    "Or, more appropriately for this situation, we can weaken the postcondition by replacing the equality assertion with an approximate equality assertion using the `pytest.approx` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13937d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_div.py\n",
    "\n",
    "from pytest import approx\n",
    "\n",
    "def div(x, y):\n",
    "    assert y != 0               # P    (precondition)\n",
    "    res = x / y                 # code (implementation)\n",
    "    assert res * y == approx(x) # Q    (postcondition)\n",
    "    return res\n",
    "\n",
    "def test_division():\n",
    "    res = div(7, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a5feea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest test_div.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1b37bf",
   "metadata": {},
   "source": [
    "***Back to the heat-equation solver. - Let's try this out!:***\n",
    "\n",
    "Let's save the `test_flux_simple` unit test in a file named `test_flux.py`. This time,\n",
    "instead of the `numpy.isclose` function, we will use `pytest.approx`, which serves\n",
    "the same purpose but provides better failure messages in unit testing contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e449b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_simple.py\n",
    "\n",
    "from heat1d import diffusive_flux\n",
    "\n",
    "def test_flux_simple():\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfd372f",
   "metadata": {},
   "source": [
    "To execute this test, we will simply run the `pytest` command in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d2116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest test_simple.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17bb9cb",
   "metadata": {},
   "source": [
    "The output confirms that the test has passed successfully, i.e., no (unexpected) assertion errors were raised during the test execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be365bac",
   "metadata": {},
   "source": [
    "### Test Discovery\n",
    "\n",
    " `pytest` automatically discovers tests by looking for files that start with `test_` or end with `_test.py`. Within each of these files, it looks for functions that start with `test_` and classes starting with `Test`. All discovered tests are then executed when you run `pytest`.\n",
    "\n",
    "\n",
    " Say, you run `pytest` in a directory with the following structure:\n",
    "\n",
    "```\n",
    "heat_solver/\n",
    "    └── heat1d.py\n",
    "    └── unit_tests/\n",
    "        ├── test_simple.py\n",
    "        └── test_flux_via_params.py\n",
    "```\n",
    "\n",
    "When you execute `pytest` from the root directory, it will recursively discover and run the tests in all the modules starting with `test_`. Since we have saved two test files so far,\n",
    "this means that both `test_simple.py` and `test_flux_via_params.py` will be executed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4381c7b1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dec734",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adbc46a",
   "metadata": {},
   "source": [
    "### Fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400ac5ac",
   "metadata": {},
   "source": [
    "\n",
    "--- live coding ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e1313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: live coding --- fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932aff40",
   "metadata": {},
   "source": [
    "--- end of live coding ---\n",
    "## Why do we use fixtures?\n",
    "\n",
    "pytest fixtures give you small, named pieces of test state, e.g., parameters, meshes, boundary conditions, that pytest builds and injects into tests by name. They remove duplicated setup, \n",
    "keep tests independent, and make intent explicit. Pytest discovers fixtures in any test file \n",
    "and in a shared `conftest.py`, so you can reuse them across modules.\n",
    "\n",
    "In this chapter we’ll use a few simple fixtures throughout:\n",
    "\n",
    " - `dx`, `kappa`: canonical numerical parameters.\n",
    " - `mesh3`, `mesh5`: tiny meshes for hand-checkable and slightly larger cases.\n",
    " - `insulated`, `linear_bc`: boundary-condition objects.\n",
    " - `F3` : Flux array sized to mesh3.\n",
    " - `u_spike`, `u_uniform`: representative initial conditions.\n",
    "\n",
    "You’ll see these fixtures appear as function arguments in the tests that follow. pytest creates them automatically and passes them in. This keeps each test concise and focused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daa15fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile conftest.py\n",
    "\"\"\"\n",
    "Shared pytest fixtures for the heat-1D solver. These create small, well-labeled\n",
    "objects we can reuse across tests without repeating setup code.\n",
    "\"\"\"\n",
    "import pytest\n",
    "from heat1d import Mesh\n",
    "\n",
    "@pytest.fixture\n",
    "def dx(): return 1.0\n",
    "\n",
    "@pytest.fixture\n",
    "def kappa(): return 0.1\n",
    "\n",
    "@pytest.fixture\n",
    "def mesh3(dx): return Mesh(dx=dx, N=3)\n",
    "\n",
    "@pytest.fixture\n",
    "def mesh5(dx): return Mesh(dx=dx, N=5)\n",
    "\n",
    "@pytest.fixture\n",
    "def insulated(): return [0.0, 0.0]\n",
    "\n",
    "@pytest.fixture\n",
    "def linear_bc(): return [1.0, 1.0]\n",
    "\n",
    "@pytest.fixture\n",
    "def F3(mesh3): return mesh3.face_field()\n",
    "\n",
    "@pytest.fixture\n",
    "def u_spike(): return [0.0, 100.0, 0.0]\n",
    "\n",
    "@pytest.fixture\n",
    "def u_uniform(mesh5): return [7.5] * mesh5.N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91981f2",
   "metadata": {},
   "source": [
    "### Marking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a058d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_fast_slow.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "def test_fast():\n",
    "    assert True\n",
    "\n",
    "def test_slow():\n",
    "    import time\n",
    "    time.sleep(3)\n",
    "    assert True\n",
    "\n",
    "def test_fail():\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bc6e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest test_fast_slow.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee561b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_fast_slow_marked.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "def test_fast():\n",
    "    assert True\n",
    "\n",
    "\n",
    "def test_slow():\n",
    "    import time\n",
    "    time.sleep(3)\n",
    "    assert True\n",
    "\n",
    "\n",
    "def test_fail():\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f4d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest test_fast_slow_marked.py -m \"xfail\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f57fe1b",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>How to register a custom mark!</summary>\n",
    "\n",
    "```\n",
    "# In a file called pytest.ini\n",
    "[pytest]\n",
    "markers =\n",
    "    slow: marks tests as slow (deselect with '-m \"not slow\"')\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a17511",
   "metadata": {},
   "source": [
    "### Parameterization\n",
    "\n",
    "You may have realized that running a test with different inputs can be tedious if we have to write separate test functions for each case. To ease this process, you can use the `@pytest.mark.parametrize` decorator to run a test function with different sets of input data.\n",
    "\n",
    "Let's return to our simple flux test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59392e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_manual_parametrize.py\n",
    "\n",
    "import pytest\n",
    "from pytest import approx\n",
    "from heat1d import diffusive_flux\n",
    "\n",
    "def test_flux_simple():\n",
    "    \"\"\"Constant field leads to zero flux\"\"\"\n",
    "    u = [100.0, 100.0, 100.0]\n",
    "    F = [0.0] * (len(u) + 1)\n",
    "\n",
    "    diffusive_flux(F, u, kappa=0.1, dx=1.0)\n",
    "    assert all(f == approx(0.0) for f in F), f\"Expected all zeros, got {F[1:-1]}\"\n",
    "\n",
    "def test_flux_non_constant():\n",
    "    \"\"\"Non-constant fields lead to non-zero flux\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa27fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest test_manual_parametrize.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7239d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_flux_via_params.py\n",
    "\n",
    "import pytest\n",
    "from pytest import approx\n",
    "from heat1d import diffusive_flux\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"u,kappa,dx,expected\",\n",
    "    [\n",
    "        ([100,100,100], 0.1, 1.0, [0.0, 0.0]),\n",
    "        ([0,10,20],     0.5, 2.0, [-0.5*(10/2), -0.5*(10/2)]),\n",
    "    ],\n",
    ")\n",
    "def test_flux_param(u, kappa, dx, expected):\n",
    "    print(f\"\\nTesting u={u}, kappa={kappa}, dx={dx}\")\n",
    "    F = [0.0]*(len(u)+1)\n",
    "    diffusive_flux(F, u, kappa, dx)\n",
    "    assert F[1:-1] == approx(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa2362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest -s test_flux_via_params.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd0076a",
   "metadata": {},
   "source": [
    "Note: The `-s` flag in the above call is to allow print statements in the test to be displayed,\n",
    "and so to confirm that all specified inputs via the parameterization mechanism are being tested.\n",
    "In the absence of this flag, the print statements are suppressed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1af41e",
   "metadata": {},
   "source": [
    "### Telescoping Property\n",
    "\n",
    "In finite-volume discretizations, fluxes between neighboring cells telescope: \n",
    "the flux leaving one cell enters the next (except in the presence of sources, \n",
    "variable cell volumes or densities, or numerical errors.)\n",
    "\n",
    "Consequently, when we sum the discrete divergence over all cells, the interior\n",
    "fluxes cancel pairwise, leaving only the boundary contributions.\n",
    "in other words, the total divergence equals the net flux through the boundaries:\n",
    "\n",
    "$\\qquad\n",
    "\\sum_{i=0}^{N-1} (\\nabla \\cdot F)_i = F_0 - F_N\n",
    "\\qquad$\n",
    "\n",
    "Recall the encoding of this property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fb09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def telescoping(c, f, dx: float) -> bool:\n",
    "    \"\"\"Check the finite volume telescoping property.\"\"\"\n",
    "    total_divergence = sum(c) * dx\n",
    "    boundary_flux = f[0] - f[-1]\n",
    "    return total_divergence == approx(boundary_flux)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd8ecde",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "#### Exercise 3.1\n",
    "\n",
    "Write a test named `test_divergence_telescopes` that verifies the telescoping property of the divergence function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7796a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_divergence.py\n",
    "\n",
    "from pytest import approx\n",
    "from heat1d import divergence\n",
    "\n",
    "def telescoping(c, f, dx: float) -> bool:\n",
    "    \"\"\"Check the finite volume telescoping property.\"\"\"\n",
    "    total_divergence = sum(c) * dx\n",
    "    boundary_flux = f[0] - f[-1]\n",
    "    return total_divergence == approx(boundary_flux)\n",
    "\n",
    "def test_divergence_telescopes(dx=1.0):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739cb2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest test_divergence.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d0f60",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Since the `divergence_telescoping` function is already incorporated in the `divergence` function, we can directly test this property by simply running it with some arbitrary inputs. This shows the value of specifying critical properties as pre- and postconditions in terms of making testing and debugging easier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a75cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_divergence.py\n",
    "\n",
    "from pytest import approx\n",
    "from heat1d import divergence\n",
    "\n",
    "def telescoping(c, f, dx: float) -> bool:\n",
    "    \"\"\"Check the finite volume telescoping property.\"\"\"\n",
    "    total_divergence = sum(c) * dx\n",
    "    boundary_flux = f[0] - f[-1]\n",
    "    return total_divergence == approx(boundary_flux)\n",
    "\n",
    "def test_divergence_telescopes(dx=1.0):\n",
    "    \"\"\"Sum of divF * dx must equal net boundary flux F[0] - F[-1].\"\"\"\n",
    "    F = [2.0, 7.0, -5.0, -3.0]\n",
    "    divF = [0.0, 0.0, 0.0]\n",
    "    divergence(divF, F, dx)\n",
    "    assert telescoping(divF, F, dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aa24fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest test_divergence.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbea1a35",
   "metadata": {},
   "source": [
    "# Step & Solve (physical invariants + stability)\n",
    "\n",
    "Finally, we check several physical invariants and stability constraints end-to-end. The purpose of each test is summarized in the docstrings(`\"\"\"...\"\"\"`) within each test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd79b74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_step_solve.py\n",
    "\n",
    "from pytest import approx, raises\n",
    "from heat1d import Mesh, step_heat_eqn, solve_heat_eqn\n",
    "\n",
    "def test_step_moves_spike_toward_neighbors(mesh3, insulated, u_spike):\n",
    "    \"\"\"Given insulated BCs and a stable dt, a single step should diffuse the spike:\n",
    "        - middle cell decreases, neighbors increase.\"\"\"\n",
    "    F = mesh3.face_field()\n",
    "    u = u_spike[:]\n",
    "    step_heat_eqn(u, kappa=0.1, dt=0.1, mesh=mesh3, bc=insulated)\n",
    "    assert u[1] < 100.0 and u[0] > 0.0 and u[2] > 0.0\n",
    "\n",
    "def test_conservation_insulated_solve(insulated):\n",
    "    \"\"\"With qL=qR=0, total discrete heat (sum(u)*dx) is invariant across step_heat_eqn.\"\"\"\n",
    "    u0 = [0.0, 100.0, 0.0]\n",
    "    u  = solve_heat_eqn(u0=u0, kappa=0.1, dt=0.1, nt=20, dx=1.0, bc=insulated)\n",
    "    assert sum(u) == approx(sum(u0))\n",
    "\n",
    "def test_conservation_with_boundary_work():\n",
    "    \"\"\"With qL!=qR, total heat changes by dt*(qL - qR) per step.\"\"\"\n",
    "    u0 = [10.0, 10.0, 10.0]\n",
    "    dx, dt, nt = 1.0, 0.05, 4\n",
    "    bc = [2.0, -3.0]  # net in = 5\n",
    "    u  = solve_heat_eqn(u0=u0, kappa=0.1, dt=dt, nt=nt, dx=dx, bc=bc)\n",
    "    expected = sum(u0)*dx + nt*dt*(bc[0] - bc[1])\n",
    "    assert sum(u)*dx == approx(expected)\n",
    "    # NOTE: bug above on purpose to show failing message; fix to bc[1] in next test.\n",
    "\n",
    "def test_symmetry_preserved_one_step(mesh3, insulated):\n",
    "    \"\"\"A symmetric initial state (a,b,a) under insulated BCs remains symmetric after 1 step.\"\"\"\n",
    "    u0 = [0.0, 100.0, 0.0]\n",
    "    u  = solve_heat_eqn(u0, kappa=0.1, dt=0.1, nt=1, dx=1.0, bc=insulated)\n",
    "    assert u[0] == approx(u[2])\n",
    "\n",
    "def test_unstable_dt_raises(insulated):\n",
    "    \"\"\" Stability guard for dx=1, kappa=0.1. Pick dt=10 to force assert.\"\"\"\n",
    "    u0 = [0.0, 100.0, 0.0]\n",
    "    with raises(AssertionError):\n",
    "        solve_heat_eqn(u0=u0, kappa=0.1, dt=10.0, nt=1, dx=1.0, bc=insulated)\n",
    "\n",
    "def test_uniform_is_fixed_point(mesh5, insulated, u_uniform):\n",
    "    \"\"\"Uniform field is a fixed point (steady state) under insulated BCs for any stable dt/kappa.\"\"\"\n",
    "    u = solve_heat_eqn(u_uniform, kappa=5.0, dt=0.05, nt=10, dx=mesh5.dx, bc=insulated)\n",
    "    assert u == approx(u_uniform)\n",
    "\n",
    "def test_equal_flux_bc_trends_toward_linear_profile(mesh5, kappa, linear_bc):\n",
    "    \"\"\" If qL==qR==c (nonzero), steady state has constant interior flux == c and thus a linear\n",
    "    gradient. This test checks that after many steps the cell differences approach a constant.\"\"\"\n",
    "    u0 = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    # stable dt: r = kappa*dt/dx^2; choose small dt to be safe\n",
    "    u  = solve_heat_eqn(u0, kappa=0.1, dt=0.5, nt=400, dx=mesh5.dx, bc=linear_bc)\n",
    "    diffs = [u[i]-u[i-1] for i in range(1, len(u))]\n",
    "    # Differences should be (approximately) equal across cells\n",
    "    avg = sum(diffs)/len(diffs)\n",
    "    assert diffs == approx([avg]*len(diffs), rel=1e-3, abs=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc54fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest test_step_solve.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181ab546",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We now have a comprehensive suite of unit tests for the 1D heat equation solver.\n",
    "We can use this test suite to validate any changes or additions to the solver's code.\n",
    "To re-run all of these tests, one can simply execute the `pytest` command.\n",
    "\n",
    "To list all available tests, the `pytest --collect-only` command can be used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c748c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --collect-only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbc35bf",
   "metadata": {},
   "source": [
    "## Limitations of Unit Testing (and how we’ll push beyond)\n",
    "\n",
    "Unit tests are necessary but not sufficient:\n",
    "\n",
    "- **Limited Coverage**: Handpicked inputs may miss edge cases or unexpected behaviors.\n",
    "- **Repetitive and Tedious**: Writing unit tests can be repetitive and tedious, especially for functions with many parameters or complex logic.\n",
    "- **Overfitting**: Tests can become too specific, making them brittle and hard to maintain. If the implementation changes, the tests may need to be rewritten, even if the overall behavior remains correct.\n",
    "\n",
    "## Looking Ahead:\n",
    "\n",
    "In Chapter 4, we’ll encode properties (conservation, symmetry, maximum-principle intuition, stability ranges) and let a generator explore many inputs automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8420c8e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "R3Sw tutorial by Alper Altuntas (NSF NCAR). Guest lecture by **Manish Venumuddula** (NSF NCAR). Sponsored by the BSSw Fellowship Program. © 2025.\n",
    "\n",
    "Cite as: Alper Altuntas, Deepak Cherian, Adrianna Foster, Manish Venumuddula, and Helen Kershaw. (2025). *\"Rigor and Reasoning in Research Software (R3Sw) Tutorial.\"* Retrieved from https://www.alperaltuntas.com/R3Sw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
