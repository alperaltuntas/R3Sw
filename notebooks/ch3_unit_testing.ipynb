{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae6580a4",
   "metadata": {},
   "source": [
    "# Chapter 3: Unit Testing\n",
    "\n",
    "## What is Unit Testing?\n",
    "\n",
    "Unit testing is a common software testing technique to check individual\n",
    "code components (e.g., functions, methods, classes, or modules) in isolation from\n",
    "the rest of the program.\n",
    "\n",
    "The main idea is to run such isolated components with a variety of inputs and check the\n",
    "outputs against expected results.\n",
    "\n",
    "For unit testing to be achievable and effective, the code design must facilitate\n",
    "easy isolation of components and their dependencies. In line with the design\n",
    "principles we discussed earlier, below are some key practices to follow:\n",
    "\n",
    " - **Small, cohesive functions:** A function should do one thing clearly (single responsibility). Recall how we applied this principle when reimplementing the heat equation solver.\n",
    "\n",
    " - **Explicit interfaces:** Functions and methods should have clear and explicit interfaces (e.g., well-defined input parameters and return values). This makes it easier to understand how to use them and to test them in isolation.\n",
    "\n",
    " - **Avoid side effects:** Functions should avoid modifying global state or relying on external state. This makes it easier to reason about their behavior and to test them in isolation.\n",
    "\n",
    " - **Use dependency injection:** Instead of hardcoding dependencies, pass them as parameters. This makes it easier to replace them with mocks or stubs during testing.  \n",
    "\n",
    " ## Benefits of Unit Testing\n",
    "\n",
    " - **Catches bugs early:** Unit tests can help identify bugs and issues in the code at an early stage.\n",
    "\n",
    " - **Facilitates code changes:** With a comprehensive suite of unit tests, developers can make changes to the code with confidence, knowing that any regressions will be caught by the tests.\n",
    "\n",
    " - **Improves code design:** Writing unit tests encourages developers to think about the design and structure of their code, leading to better-organized and more maintainable code.\n",
    "\n",
    " - **Documentation:** Unit tests serve as a form of documentation, providing examples of how to use the code and what its expected behavior is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ad44dd",
   "metadata": {},
   "source": [
    "\n",
    "## Unit Testing in Action\n",
    "\n",
    "Take the `diffusive_flux` function from the previous chapter as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b38c35ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = list[float]\n",
    "\n",
    "def diffusive_flux(f_out: vec, c: vec, kappa: float, dx: float) -> None:\n",
    "    \"\"\"Given a cell field (c), compute the diffusive flux (f_out).\"\"\"\n",
    "    assert len(f_out) == len(c) + 1, \"Size mismatch\"\n",
    "    assert dx > 0 and kappa > 0, \"Non-positive dx or kappa\"\n",
    "    for i in range(1, len(f_out) - 1):\n",
    "        f_out[i] = -kappa * (c[i] - c[i-1]) / dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb3570",
   "metadata": {},
   "source": [
    "Unit testing this function is as simple as calling it with some test inputs and checking the outputs. Here's how you might write a unit test for the `diffusive_flux` function using `pytest`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f400961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import isclose\n",
    "\n",
    "def test_diffusive_flux():\n",
    "    \"\"\"Constant field leads to zero flux\"\"\"\n",
    "    u = [100.0, 100.0, 100.0]\n",
    "    F = [0.0] * (len(u) + 1)\n",
    "\n",
    "    diffusive_flux(F, u, kappa=0.1, dx=1.0)\n",
    "    assert all(isclose(F[1:-1], 0)), f\"Expected all zeros, got {F[1:-1]}\"\n",
    "\n",
    "test_diffusive_flux()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e3926b",
   "metadata": {},
   "source": [
    "That's it! We unit tested the `diffusive_flux` function. And the test passed successfully.\n",
    "How do we know? The assertion did not raise an error.\n",
    "\n",
    "But we only did so for a specific case, where we confirmed that a constant field \n",
    "and zero boundary fluxes lead to zero flux. We'll add more tests to cover different scenarios,\n",
    "but first let's introduce the `pytest` framework, which makes writing and running tests easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571498fa",
   "metadata": {},
   "source": [
    "## The `pytest` Library\n",
    "\n",
    "While manual testing is useful, it can be time-consuming and error-prone. Automated testing with a framework like `pytest` allows us to quickly and easily run our tests, for instance, when added into a continuous integration (CI) pipeline.\n",
    "\n",
    "While there are other commonly used frameworks such as `unittest`, we prefer\n",
    "`pytest` for its simplicity and powerful features. It's also worth noting that `pytest`\n",
    "is not just for unit testing: it's a general purpose testing framework that can be used\n",
    "for a wide range of testing needs in an automated fashion.\n",
    "\n",
    "Note that `pytest` is a command-line tool. As such, we will follow the following\n",
    "workflow to run our tests in a Jupyter notebook environment:\n",
    "\n",
    "1. Using the `%%writefile` magic command, we will save our test codes to Python files.\n",
    "2. We will then run the tests using the command line command `pytest`. Recall, in Jupyter notebooks, we can run such shell commands by prefixing them with `!`.\n",
    "\n",
    "But first, let's load the solver code that was saved in the previous chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3067e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load heat1d.py\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pytest import approx\n",
    "\n",
    "vec = list[float]\n",
    "\n",
    "@dataclass\n",
    "class Mesh:\n",
    "    \"\"\"Uniform 1-D mesh.\"\"\"\n",
    "\n",
    "    dx: float  # cell size\n",
    "    N: int     # number of cells\n",
    "\n",
    "    def cell_field(self) -> vec:\n",
    "        return [0.0] * self.N\n",
    "\n",
    "    def face_field(self) -> vec:\n",
    "        return [0.0] * (self.N + 1)\n",
    "\n",
    "def apply_bc(f_out: vec, bc: vec) -> None:\n",
    "    \"\"\"Apply BCs by overriding first and last face quantities (f_out).\"\"\"\n",
    "    assert len(f_out) > 1, \"face field size too small\"\n",
    "    assert len(bc) == 2, \"bc must be of size 2\"\n",
    "    f_out[0], f_out[-1] = bc[0], bc[1]\n",
    "\n",
    "def diffusive_flux(f_out: vec, c: vec, kappa: float, dx: float) -> None:\n",
    "    \"\"\"Given a cell field (c), compute the diffusive flux (f_out).\"\"\"\n",
    "    assert len(f_out) == len(c) + 1, \"Size mismatch\"\n",
    "    assert dx > 0 and kappa > 0, \"Non-positive dx or kappa\"\n",
    "    for i in range(1, len(f_out) - 1):\n",
    "        f_out[i] = -kappa * (c[i] - c[i-1]) / dx\n",
    "\n",
    "def divergence(c_out: vec, f: vec, dx: float) -> None:\n",
    "    \"\"\"Compute the divergence of face quantities (f) and store in (c_out).\"\"\"\n",
    "    assert len(c_out) == len(f) - 1, \"Size mismatch\"\n",
    "    assert dx > 0, \"Non-positive dx\"\n",
    "    for i in range(len(c_out)):\n",
    "        c_out[i] = (f[i] - f[i+1]) / dx\n",
    "\n",
    "def step_heat_eqn(u_inout: vec, kappa: float, dt: float, mesh: Mesh, bc: vec):\n",
    "    \"\"\"Advance cell field u by one time step using explicit Euler method.\"\"\"\n",
    "    assert dt > 0, \"Non-positive dt\"\n",
    "    assert mesh.N == len(u_inout), \"Size mismatch\"\n",
    "\n",
    "    F = mesh.face_field()\n",
    "    divF = mesh.cell_field()\n",
    "\n",
    "    apply_bc(F, bc)\n",
    "    diffusive_flux(F, u_inout, kappa, mesh.dx)\n",
    "    divergence(divF, F, mesh.dx)\n",
    "\n",
    "    for i in range(mesh.N):\n",
    "        u_inout[i] += dt * divF[i]\n",
    "\n",
    "def solve_heat_eqn(u0: vec, kappa: float, dt: float, nt: int, dx: float, bc: vec) -> vec:\n",
    "    \"\"\"Orchestrate nt steps over cell field u.\"\"\"\n",
    "\n",
    "    assert nt > 0, \"Number of time steps must be positive\"\n",
    "    assert dt <= (dx ** 2) / (2 * kappa), \"Stability condition not met\"\n",
    "\n",
    "    mesh = Mesh(dx, N=len(u0))\n",
    "    u = u0.copy()\n",
    "    for _ in range(nt):\n",
    "        step_heat_eqn(u, kappa, dt, mesh, bc)\n",
    "\n",
    "    return u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773035ee",
   "metadata": {},
   "source": [
    "## Key Features of `pytest`\n",
    "\n",
    "### Assertions\n",
    "\n",
    "pytest uses plain Python assert statements, no special API, to decide whether a test passes.\n",
    "It also rewrites assertions to show the full expression and the actual values on failure, \n",
    "which makes errors easy to diagnose. When you expect an error, use the `pytest.raises` context manager to assert that a specific exception is raised.\n",
    "\n",
    "Be careful with floating-point comparisons: exact equality is brittle. In tests, \n",
    "prefer `pytest.approx` for tolerant comparisons. Using `numpy.isclose` inside your\n",
    "library code is preferable in production code. But for test assertions,\n",
    "`approx` tends to produce clearer failure messages.\n",
    "\n",
    "Because we embedded pre- and postconditions as asserts in our solver, those checks run \n",
    "automatically whenever tests execute the code. Just remember they only fire for the specific\n",
    "inputs your tests provide. On top of those contracts, you’ll typically add test-side assertions\n",
    "that express the behavior you want to verify, like the semantic assertions we discussed earlier.\n",
    "\n",
    "Pytest discovers tests by looking for functions whose names start with `test_`. Each such\n",
    "function is executed as an independent test: if an assertion fails, pytest reports the failure\n",
    "(with values); if an unexpected exception occurs, it reports that too.\n",
    "\n",
    "Let’s write a minimal test for the `div` function. In practice, source code and test are \n",
    "often located in separate files. But for brevity, we’ll keep them together and save to\n",
    "`test_div.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04219b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_div.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_div.py\n",
    "\n",
    "def div(x, y):\n",
    "    assert y != 0           # P    (precondition)\n",
    "    res = x / y             # code (implementation)\n",
    "    assert res * y == x     # Q    (postcondition)\n",
    "    return res\n",
    "\n",
    "def test_division():\n",
    "    div(7, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f64a60",
   "metadata": {},
   "source": [
    "To run this test, we can simply run the `pytest test_div.py` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9530cd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0\n",
      "rootdir: /Users/altuntas/r3sw/notebooks\n",
      "plugins: hypothesis-6.136.9, anyio-4.10.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_div.py \u001b[31mF\u001b[0m\u001b[31m                                                            [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m________________________________ test_division _________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_division\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       div(\u001b[94m7\u001b[39;49;00m, \u001b[94m25\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_div.py\u001b[0m:9: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "x = 7, y = 25\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mdiv\u001b[39;49;00m(x, y):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m y != \u001b[94m0\u001b[39;49;00m           \u001b[90m# P    (precondition)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        res = x / y             \u001b[90m# code (implementation)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m res * y == x     \u001b[90m# Q    (postcondition)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert (0.28 * 25) == 7\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_div.py\u001b[0m:5: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_div.py::\u001b[1mtest_division\u001b[0m - assert (0.28 * 25) == 7\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.31s\u001b[0m\u001b[31m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_div.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d069e85a",
   "metadata": {},
   "source": [
    "Notice the test fails because the assertion is violated due to floating point precision of division operation. We may address this test failure in several ways. First, we can\n",
    "mark the test as expected to fail using the `@pytest.mark.xfail` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8637461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_div.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_div.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "def div(x, y):\n",
    "    assert y != 0           # P    (precondition)\n",
    "    res = x / y             # code (implementation)\n",
    "    assert res * y == x     # Q    (postcondition)\n",
    "    return res\n",
    "\n",
    "@pytest.mark.xfail\n",
    "def test_division():\n",
    "    res = div(7, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3917166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0\n",
      "rootdir: /Users/altuntas/r3sw/notebooks\n",
      "plugins: hypothesis-6.136.9, anyio-4.10.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_div.py \u001b[33mx\u001b[0m\u001b[33m                                                            [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m============================== \u001b[33m\u001b[1m1 xfailed\u001b[0m\u001b[33m in 0.20s\u001b[0m\u001b[33m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_div.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d7707d",
   "metadata": {},
   "source": [
    "Alternatively, we can use the `raises` context manager:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aee3bf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_div.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_div.py\n",
    "\n",
    "from pytest import raises\n",
    "\n",
    "def div(x, y):\n",
    "    assert y != 0           # P    (precondition)\n",
    "    res = x / y             # code (implementation)\n",
    "    assert res * y == x     # Q    (postcondition)\n",
    "    return res\n",
    "\n",
    "def test_division():\n",
    "    with raises(AssertionError):\n",
    "        res = div(7, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "952cbaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0\n",
      "rootdir: /Users/altuntas/r3sw/notebooks\n",
      "plugins: hypothesis-6.136.9, anyio-4.10.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_div.py \u001b[32m.\u001b[0m\u001b[32m                                                            [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.19s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_div.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7452eaf",
   "metadata": {},
   "source": [
    "Or, more appropriately for this situation, we can weaken the postcondition by replacing the equality assertion with an approximate equality assertion using the `pytest.approx` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e13937d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_div.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_div.py\n",
    "\n",
    "from pytest import approx\n",
    "\n",
    "def div(x, y):\n",
    "    assert y != 0               # P    (precondition)\n",
    "    res = x / y                 # code (implementation)\n",
    "    assert res * y == approx(x) # Q    (postcondition)\n",
    "    return res\n",
    "\n",
    "def test_division():\n",
    "    res = div(7, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6a5feea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0\n",
      "rootdir: /Users/altuntas/r3sw/notebooks\n",
      "plugins: hypothesis-6.136.9, anyio-4.10.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_div.py \u001b[32m.\u001b[0m\u001b[32m                                                            [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.29s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_div.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1b37bf",
   "metadata": {},
   "source": [
    "Back to the heat-equation solver: execute the following cell to save this minimal test \n",
    "definition as `test_simple.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9e449b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_simple.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_simple.py\n",
    "\n",
    "import pytest\n",
    "from heat1d import diffusive_flux\n",
    "\n",
    "def test_flux_simple():\n",
    "    u = [100, 100, 100]\n",
    "    kappa = 0.1\n",
    "    dx = 1.0\n",
    "    F = [0.0] * (len(u) + 1)\n",
    "    diffusive_flux(F, u, kappa, dx)\n",
    "    assert F[1:-1] == pytest.approx([0.0, 0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfd372f",
   "metadata": {},
   "source": [
    "To execute this test, we will simply run the `pytest` command in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4d2116d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0\n",
      "rootdir: /Users/altuntas/r3sw/notebooks\n",
      "plugins: hypothesis-6.136.9, anyio-4.10.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_simple.py \u001b[32m.\u001b[0m\u001b[32m                                                         [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.29s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_simple.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17bb9cb",
   "metadata": {},
   "source": [
    "The output confirms that the test has passed successfully, i.e., no (unexpected) assertion errors were raised during the test execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b7fc95",
   "metadata": {},
   "source": [
    "\n",
    "### Parameterization\n",
    "\n",
    "You may have realized that running a test with different inputs can be tedious if we have to write separate test functions for each case. To ease this process, you can use the `@pytest.mark.parametrize` decorator to run a test function with different sets of input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d24d5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_flux_via_params.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_flux_via_params.py\n",
    "\n",
    "import pytest\n",
    "from pytest import approx\n",
    "from heat1d import diffusive_flux\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"u,kappa,dx,expected\",\n",
    "    [\n",
    "        ([100,100,100], 0.1, 1.0, [0.0, 0.0]),\n",
    "        ([0,10,20],     0.5, 2.0, [-0.5*(10/2), -0.5*(10/2)]),\n",
    "    ],\n",
    "    ids=[\"constant→zero\", \"linear→const-flux\"]\n",
    ")\n",
    "def test_flux_param(u, kappa, dx, expected):\n",
    "    print(f\"\\nTesting u={u}, kappa={kappa}, dx={dx}\")\n",
    "    F = [0.0]*(len(u)+1)\n",
    "    diffusive_flux(F, u, kappa, dx)\n",
    "    assert F[1:-1] == approx(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47389c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0\n",
      "rootdir: /Users/altuntas/r3sw/notebooks\n",
      "plugins: hypothesis-6.136.9, anyio-4.10.0\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "test_flux_via_params.py \n",
      "Testing u=[100, 100, 100], kappa=0.1, dx=1.0\n",
      "\u001b[32m.\u001b[0m\n",
      "Testing u=[0, 10, 20], kappa=0.5, dx=2.0\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.29s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -s test_flux_via_params.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351f728b",
   "metadata": {},
   "source": [
    "Note: The `-s` flag in the above call is to allow print statements in the test to be displayed,\n",
    "and so to confirm that all specified inputs via the parameterization mechanism are being tested.\n",
    "In the absence of this flag, the print statements are suppressed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be365bac",
   "metadata": {},
   "source": [
    "### Test Discovery\n",
    "\n",
    " `pytest` automatically discovers tests by looking for files that start with `test_` or end with `_test.py`. Within each of these files, it looks for functions that start with `test_` and classes starting with `Test`. All discovered tests are then executed when you run `pytest`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4381c7b1",
   "metadata": {},
   "source": [
    "\n",
    " Say, you run `pytest` in a directory with the following structure:\n",
    "\n",
    "```\n",
    "heat_solver/\n",
    "    └── heat1d.py\n",
    "    └── unit_tests/\n",
    "        ├── test_simple.py\n",
    "        └── test_flux_via_params.py\n",
    "```\n",
    "\n",
    "When you execute `pytest` from the root directory, it will recursively discover and run the tests in all the modules starting with `test_`. Since we have saved two test files so far,\n",
    "this means that both `test_simple.py` and `test_flux_via_params.py` will be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95dec734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0\n",
      "rootdir: /Users/altuntas/r3sw/notebooks\n",
      "plugins: hypothesis-6.136.9, anyio-4.10.0\n",
      "collected 14 items                                                             \u001b[0m\n",
      "\n",
      "test_div.py \u001b[32m.\u001b[0m\u001b[32m                                                            [  7%]\u001b[0m\n",
      "test_divergence.py \u001b[32m.\u001b[0m\u001b[32m                                                     [ 14%]\u001b[0m\n",
      "test_flux.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                          [ 28%]\u001b[0m\n",
      "test_flux_via_params.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                               [ 42%]\u001b[0m\n",
      "test_simple.py \u001b[32m.\u001b[0m\u001b[32m                                                         [ 50%]\u001b[0m\n",
      "test_step_solve.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                               [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m14 passed\u001b[0m\u001b[32m in 0.44s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adbc46a",
   "metadata": {},
   "source": [
    "\n",
    "### Fixtures\n",
    "\n",
    "pytest fixtures give you small, named pieces of test state, e.g., parameters, meshes, boundary conditions, that pytest builds and injects into tests by name. They remove duplicated setup, \n",
    "keep tests independent, and make intent explicit. Pytest discovers fixtures in any test file \n",
    "and in a shared `conftest.py`, so you can reuse them across modules.\n",
    "\n",
    "In this chapter we’ll use a few simple fixtures throughout:\n",
    "\n",
    " - `dx`, `kappa`: canonical numerical parameters.\n",
    " - `mesh3`, `mesh5`: tiny meshes for hand-checkable and slightly larger cases.\n",
    " - `insulated`, `linear_bc`: boundary-condition objects.\n",
    " - `F3`, `dudt3`: Flux and tendency arrays sized to mesh3.\n",
    " - `u_spike`, `u_uniform`: representative initial conditions.\n",
    "\n",
    "You’ll see these fixtures appear as function arguments in the tests that follow. pytest creates them automatically and passes them in. This keeps each test concise and focused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1daa15fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting conftest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile conftest.py\n",
    "\"\"\"\n",
    "Shared pytest fixtures for the heat-1D solver. These create small, well-labeled\n",
    "objects we can reuse across tests without repeating setup code.\n",
    "\"\"\"\n",
    "import pytest\n",
    "from heat1d import Mesh\n",
    "\n",
    "@pytest.fixture\n",
    "def dx(): return 1.0\n",
    "\n",
    "@pytest.fixture\n",
    "def kappa(): return 0.1\n",
    "\n",
    "@pytest.fixture\n",
    "def mesh3(dx): return Mesh(dx=dx, N=3)\n",
    "\n",
    "@pytest.fixture\n",
    "def mesh5(dx): return Mesh(dx=dx, N=5)\n",
    "\n",
    "@pytest.fixture\n",
    "def insulated(): return [0.0, 0.0]\n",
    "\n",
    "@pytest.fixture\n",
    "def linear_bc(): return [1.0, 1.0]\n",
    "\n",
    "@pytest.fixture\n",
    "def F3(mesh3): return mesh3.face_field()\n",
    "\n",
    "@pytest.fixture\n",
    "def dudt3(mesh3): return mesh3.cell_field()\n",
    "\n",
    "@pytest.fixture\n",
    "def u_spike(): return [0.0, 100.0, 0.0]\n",
    "\n",
    "@pytest.fixture\n",
    "def u_uniform(mesh5): return [7.5] * mesh5.N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf37756",
   "metadata": {},
   "source": [
    "## More examples\n",
    "\n",
    "### Flux tests\n",
    "\n",
    "Below are a couple of tests for the flux function.\n",
    "\n",
    "In `test_flux_constant_field_yields_zero_interior`, we show that a constant temperature field\n",
    "has zero interior gradients, so the interior fluxes F[1:-1] must be zero. Boundary faces are\n",
    "governed by boundary conditions and are intentionally left untouched by flux.\n",
    "\n",
    "In `test_flux_spike_has_opposite_signed_fluxes`, the spike profile [0, 100, 0] produces a\n",
    "positive gradient at the first interior face and a negative gradient at the second, so the \n",
    "interior fluxes must have equal magnitude and opposite sign (i.e., [-κ·100/Δx, +κ·100/Δx]). \n",
    "Using the `F3` and `u_spike` fixtures keeps the setup clear and state isolated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4584d298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_flux.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_flux.py\n",
    "from pytest import approx, raises\n",
    "from heat1d import diffusive_flux\n",
    "\n",
    "def test_flux_constant_field_yields_zero_interior():\n",
    "    \"\"\"Given a constant field, interior gradients are zero → interior fluxes are zero.\"\"\"\n",
    "    u = [100.0, 100.0, 100.0]\n",
    "    F = [9.9, 0.0, 0.0, 8.8]  # sentinels at boundaries to show they're not changed\n",
    "    diffusive_flux(F, u, kappa=0.1, dx=1.0)\n",
    "    assert F[1:-1] == approx([0.0, 0.0])\n",
    "    assert F[0] == 9.9 and F[-1] == 8.8  # boundary faces untouched by flux()\n",
    "\n",
    "def test_flux_spike_has_opposite_signed_fluxes(F3, u_spike):\n",
    "    \"\"\"A positive jump then negative jump should produce equal/opposite interior fluxes.\"\"\"\n",
    "    kappa, dx = 0.1, 1.0\n",
    "    F = F3[:]\n",
    "    diffusive_flux(F, u_spike, kappa, dx)\n",
    "    assert F[1:-1] == approx([-kappa*100.0, +kappa*100.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24d19b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0\n",
      "rootdir: /Users/altuntas/r3sw/notebooks\n",
      "plugins: hypothesis-6.136.9, anyio-4.10.0\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "test_flux.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                          [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.29s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_flux.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1af41e",
   "metadata": {},
   "source": [
    "### Divergence telescoping\n",
    "\n",
    "\n",
    "The finite volume telescoping property states that summing the tendencies (`dudt[i]`) over all cells and multiplying by `dx` must equal the net boundary flux (`F[0] - F[-1]`).\n",
    "\n",
    "Recall that we have the below function as a postcondition after each time `divergence` is computed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5fb09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def telescoping(c, f, dx: float) -> bool:\n",
    "    \"\"\"Check the finite volume telescoping property.\"\"\"\n",
    "    total_divergence = sum(c) * dx\n",
    "    boundary_flux = f[0] - f[-1]\n",
    "    return total_divergence == approx(boundary_flux)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd8ecde",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "Therefore, running the `divergence` functions for arbitrary inputs automatically verifies the telescoping property.\n",
    "\n",
    "#### Exercise 3.1\n",
    "\n",
    "Write a test named `test_divergence_telescopes` that verifies the telescoping property of the divergence function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7796a518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_divergence.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_divergence.py\n",
    "\n",
    "from pytest import approx\n",
    "from heat1d import divergence\n",
    "\n",
    "\n",
    "def test_divergence_telescopes(dx=1.0):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "739cb2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0\n",
      "rootdir: /Users/altuntas/r3sw/notebooks\n",
      "plugins: hypothesis-6.136.9, anyio-4.10.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_divergence.py \u001b[32m.\u001b[0m\u001b[32m                                                     [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.18s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_divergence.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d0f60",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Since the `divergence_telescoping` function is already incorporated in the `divergence` function, we can directly test this property by simply running it with some arbitrary inputs. This shows the value of specifying critical properties as pre- and postconditions in terms of making testing and debugging easier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72a75cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_divergence.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_divergence.py\n",
    "\n",
    "from pytest import approx\n",
    "from heat1d import divergence\n",
    "\n",
    "def test_divergence_telescopes(dx=1.0):\n",
    "    \"\"\"Sum of dudt * dx must equal net boundary flux F[0] - F[-1].\"\"\"\n",
    "    F = [2.0, 7.0, -5.0, -3.0]  # N=3 → dudt length=3\n",
    "    dudt = [0.0, 0.0, 0.0]\n",
    "    divergence(dudt, F, dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3aa24fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0\n",
      "rootdir: /Users/altuntas/r3sw/notebooks\n",
      "plugins: hypothesis-6.136.9, anyio-4.10.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_divergence.py \u001b[32m.\u001b[0m\u001b[32m                                                     [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.17s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_divergence.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbea1a35",
   "metadata": {},
   "source": [
    "# Step & Solve (physical invariants + stability)\n",
    "\n",
    "Finally, we check several physical invariants and stability constraints end-to-end. The purpose of each test is summarized in the docstrings(`\"\"\"...\"\"\"`) within each test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd79b74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_step_solve.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_step_solve.py\n",
    "\n",
    "from pytest import approx, raises\n",
    "from heat1d import Mesh, step_heat_eqn, solve_heat_eqn\n",
    "\n",
    "def test_step_moves_spike_toward_neighbors(mesh3, insulated, u_spike):\n",
    "    \"\"\"Given insulated BCs and a stable dt, a single step should diffuse the spike:\n",
    "        - middle cell decreases, neighbors increase.\"\"\"\n",
    "    F = mesh3.face_field()\n",
    "    dudt = mesh3.cell_field()\n",
    "    u = u_spike[:]\n",
    "    step_heat_eqn(u, kappa=0.1, dt=0.1, mesh=mesh3, bc=insulated)\n",
    "    assert u[1] < 100.0 and u[0] > 0.0 and u[2] > 0.0\n",
    "\n",
    "def test_conservation_insulated_solve(insulated):\n",
    "    \"\"\"With qL=qR=0, total discrete heat (sum(u)*dx) is invariant across step_heat_eqn.\"\"\"\n",
    "    u0 = [0.0, 100.0, 0.0]\n",
    "    u  = solve_heat_eqn(u0=u0, kappa=0.1, dt=0.1, nt=20, dx=1.0, bc=insulated)\n",
    "    assert sum(u) == approx(sum(u0))\n",
    "\n",
    "def test_conservation_with_boundary_work():\n",
    "    \"\"\"With qL!=qR, total heat changes by dt*(qL - qR) per step.\"\"\"\n",
    "    u0 = [10.0, 10.0, 10.0]\n",
    "    dx, dt, nt = 1.0, 0.05, 4\n",
    "    bc = [2.0, -3.0]  # net in = 5\n",
    "    u  = solve_heat_eqn(u0=u0, kappa=0.1, dt=dt, nt=nt, dx=dx, bc=bc)\n",
    "    expected = sum(u0)*dx + nt*dt*(bc[0] - bc[1])\n",
    "    assert sum(u)*dx == approx(expected)\n",
    "    # NOTE: bug above on purpose to show failing message; fix to bc[1] in next test.\n",
    "\n",
    "def test_symmetry_preserved_one_step(mesh3, insulated):\n",
    "    \"\"\"A symmetric initial state (a,b,a) under insulated BCs remains symmetric after 1 step.\"\"\"\n",
    "    u0 = [0.0, 100.0, 0.0]\n",
    "    u  = solve_heat_eqn(u0, kappa=0.1, dt=0.1, nt=1, dx=1.0, bc=insulated)\n",
    "    assert u[0] == approx(u[2])\n",
    "\n",
    "def test_unstable_dt_raises(insulated):\n",
    "    \"\"\" Stability guard for dx=1, kappa=0.1. Pick dt=10 to force assert.\"\"\"\n",
    "    u0 = [0.0, 100.0, 0.0]\n",
    "    with raises(AssertionError):\n",
    "        solve_heat_eqn(u0=u0, kappa=0.1, dt=10.0, nt=1, dx=1.0, bc=insulated)\n",
    "\n",
    "def test_uniform_is_fixed_point(mesh5, insulated, u_uniform):\n",
    "    \"\"\"Uniform field is a fixed point (steady state) under insulated BCs for any stable dt/kappa.\"\"\"\n",
    "    u = solve_heat_eqn(u_uniform, kappa=5.0, dt=0.05, nt=10, dx=mesh5.dx, bc=insulated)\n",
    "    assert u == approx(u_uniform)\n",
    "\n",
    "\n",
    "def test_equal_flux_bc_trends_toward_linear_profile(mesh5, kappa, linear_bc):\n",
    "    \"\"\" If qL==qR==c (nonzero), steady state has constant interior flux == c and thus a linear\n",
    "    gradient. This test checks that after many steps the cell differences approach a constant.\"\"\"\n",
    "    u0 = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    # stable dt: r = kappa*dt/dx^2; choose small dt to be safe\n",
    "    u  = solve_heat_eqn(u0, kappa=0.1, dt=0.5, nt=400, dx=mesh5.dx, bc=linear_bc)\n",
    "    diffs = [u[i]-u[i-1] for i in range(1, len(u))]\n",
    "    # Differences should be (approximately) equal across cells\n",
    "    avg = sum(diffs)/len(diffs)\n",
    "    assert diffs == approx([avg]*len(diffs), rel=1e-3, abs=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cefc54fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0\n",
      "rootdir: /Users/altuntas/r3sw/notebooks\n",
      "plugins: hypothesis-6.136.9, anyio-4.10.0\n",
      "collected 7 items                                                              \u001b[0m\n",
      "\n",
      "test_step_solve.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                               [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m7 passed\u001b[0m\u001b[32m in 0.30s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_step_solve.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181ab546",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We now have a comprehensive suite of unit tests for the 1D heat equation solver.\n",
    "We can use this test suite to validate any changes or additions to the solver's code.\n",
    "To re-run all of these tests, one can simply execute the `pytest` command.\n",
    "\n",
    "To list all available tests, the `pytest --collect-only` command can be used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c748c9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0\n",
      "rootdir: /Users/altuntas/r3sw/notebooks\n",
      "plugins: hypothesis-6.136.9, anyio-4.10.0\n",
      "collected 14 items                                                             \u001b[0m\n",
      "\n",
      "<Dir notebooks>\n",
      "  <Module test_div.py>\n",
      "    <Function test_division>\n",
      "  <Module test_divergence.py>\n",
      "    <Function test_divergence_telescopes>\n",
      "  <Module test_flux.py>\n",
      "    <Function test_flux_constant_field_yields_zero_interior>\n",
      "    <Function test_flux_spike_has_opposite_signed_fluxes>\n",
      "  <Module test_flux_via_params.py>\n",
      "    <Function test_flux_param[constant\\u2192zero]>\n",
      "    <Function test_flux_param[linear\\u2192const-flux]>\n",
      "  <Module test_simple.py>\n",
      "    <Function test_flux_simple>\n",
      "  <Module test_step_solve.py>\n",
      "    <Function test_step_moves_spike_toward_neighbors>\n",
      "    <Function test_conservation_insulated_solve>\n",
      "    <Function test_conservation_with_boundary_work>\n",
      "    <Function test_symmetry_preserved_one_step>\n",
      "    <Function test_unstable_dt_raises>\n",
      "    <Function test_uniform_is_fixed_point>\n",
      "    <Function test_equal_flux_bc_trends_toward_linear_profile>\n",
      "\n",
      "\u001b[32m========================= \u001b[32m14 tests collected\u001b[0m\u001b[32m in 0.27s\u001b[0m\u001b[32m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest --collect-only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbc35bf",
   "metadata": {},
   "source": [
    "## Limitations of Unit Testing (and how we’ll push beyond)\n",
    "\n",
    "Unit tests are necessary but not sufficient:\n",
    "\n",
    "- **Limited Coverage**: Handpicked inputs may miss edge cases or unexpected behaviors.\n",
    "- **Repetitive and Tedious**: Writing unit tests can be repetitive and tedious, especially for functions with many parameters or complex logic.\n",
    "- **Overfitting**: Tests can become too specific, making them brittle and hard to maintain. If the implementation changes, the tests may need to be rewritten, even if the overall behavior remains correct.\n",
    "\n",
    "## Looking Ahead:\n",
    "\n",
    "In Chapter 4, we’ll encode properties (conservation, symmetry, maximum-principle intuition, stability ranges) and let a generator explore many inputs automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8420c8e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "R3Sw tutorial by **Alper Altuntas** (NSF NCAR). Guest lecture by **Manish Venumuddula** (NSF NCAR). Sponsored by the BSSw Fellowship Program. © 2025.\n",
    "\n",
    "Cite as: Alper Altuntas, Philip Zucker, Deepak Cherian, Adrianna Foster, Manish Venumuddula, and Helen Kershaw. (2025). *\"Rigor and Reasoning in Research Software (R3Sw) Tutorial.\"* https://www.alperaltuntas.com/R3Sw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r3sw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
